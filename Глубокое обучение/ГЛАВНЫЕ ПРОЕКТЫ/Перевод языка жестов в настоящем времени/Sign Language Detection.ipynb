{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8beed120",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66972ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7443cbf",
   "metadata": {},
   "source": [
    "# Создание Keypoints с помощью MediaPipe Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9782aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Создает модель для инициализирования keypoints (ключевые точки)\n",
    "mp_drawing = mp.solutions.drawing_utils # Утилиты для того, чтобы модель могла рисовать точки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d191ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#модель на вход получает BGR изображение, но чтобы установить точки нужна конвертация в RGB\n",
    "def mediapipe_detection(image, model): \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB, чтобы мы могли установить точки\n",
    "    #cvtColor - позволяет конвертировать изображение из одного цветого измерения в другое  \n",
    "    image.flags.writeable = False                  # Image is no longer writeable, чтобы сохранить память\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc5271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #напишем функцию, которая позволит увидеть landmarks\n",
    "#Данная функция являеется упрощенной, ниже мы напишем фукнцию, которая лучше подходить под индивидуальные части тела\n",
    "# def draw_landmarks(image, results):\n",
    "#     #results.BODY_PART_landmarks - Точки, которые есть на части тела\n",
    "#     #mp_holistic.BODY_PART_CONNECTIONS - указывает к какой части тела привязаны данные точки (Пр. внутренности правого глаза\n",
    "#     #привязаны к правому глазу) \n",
    "#     mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Рисует соединения в лице\n",
    "#     mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Рисует соединения в позе\n",
    "#     mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Рисует соединения в левой руке\n",
    "#     mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Рисует соединения в правой руке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7812cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#улучшнная фукнция для рисования точек \n",
    "#Происхоидт все тоже самое, что и в упрощенной функции, однако добавляются два дополнтильеных параметра\n",
    "#mp_drawing.DrawingSpec\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), #изменяет landmarks\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #изменят соединения\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35a6d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0) #захват видеокамеры с индексом 0\n",
    "\n",
    "# #Получение доступа к mediapipe модели. Таким образом мы можем использовать функции данного класса в цикле.\n",
    "# #Mediapipe работает таким образом, что сначала происхоит базовая детекция, а потом ищутся ключевые точки\n",
    "# #min_detection_confidence - первоначальная детекция\n",
    "# #min_tracking_confidence - второстепенный поиск ключевых точек\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened(): #пока видеокамера работает\n",
    "\n",
    "#         ret, frame = cap.read() #возвращает два аргумента. Первый говорит о том, успешно ли удалось подключаться к камере\n",
    "#         #второй аргумнет возвращает изображение\n",
    "\n",
    "#         #Создает детекцию точек\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "#         #results хранит в себе данные о landmarks (положении точек, которые нашла модель)\n",
    "#         #на положение точек можно посмотреть с помощью results. и дальше будет список \n",
    "        \n",
    "#         #Рисуем соединения между точками\n",
    "#         draw_styled_landmarks(image, results)\n",
    "\n",
    "#         cv2.imshow('OpenCV Feed', image) #позволяет увидеть изображение в окне, которое появляется на экране\n",
    "\n",
    "#         #Если на протяжении 10 миллсек была нажата клавиша 'q'\n",
    "#         #то луп обрывается\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'): \n",
    "#             break\n",
    "\n",
    "#     cap.release() #отпускает ресурсы видеокамеры\n",
    "#     cv2.destroyAllWindows() #убираем окна  изображением "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289f128",
   "metadata": {},
   "source": [
    "# Выгрузка значенй Keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32444a",
   "metadata": {},
   "source": [
    "Напишем функцию, которая возвращает значения считанных координат с каждой части тела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84460bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    #If условие нам нужно в том случае, если жест не будет включать какую-либо часть тела. В таком случае камера не зафиксирует\n",
    "    #данную часть тела, по скольку мы её на покажем. Если мы попытаемся взять значения для этой части тела, то получим ошибку \n",
    "    #В данном случае нам нужно будет заполнить array для данной части тела нулями в форме N (количество точек для) части\n",
    "    #тела * N координат.\n",
    "    #К примеру Всего в туловище у нас имеются 33 точки, которые содержат по 4 координаты (x, y, z, visibility).\n",
    "    #Таким образом, если мы не покажем\n",
    "    #камере туловище, то мы заполним array для данной части тела нулевым array в форме 33 * 4. Почему не просто 33 строки по\n",
    "    #3 колонки? Потому что мы будем давать нейросети на вход flatten данные.\n",
    "    #Flaten означает, что array переходит в 1-мерное измерение. \n",
    "\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    #После этого возвращаем объединенный array, который содержит точки всех частей тела.\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76fdea",
   "metadata": {},
   "source": [
    "# Назначаем пути для обучающих данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152ec8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пути для экспортирования данных extract_keypoints()\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Действия которые мы хотим распознать\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "# Количество видео для каждого действия, которые надо заснять\n",
    "no_sequences = 30\n",
    "\n",
    "# Каждое видео будет 30 кадров в длину \n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489bc4e2",
   "metadata": {},
   "source": [
    "Напишем цикл, который создает папку для каждого действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2097e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello - папка с действием\n",
    "## 0 - видео №\n",
    "### 0 - Кадр №\n",
    "### 1 - Кадр №\n",
    "### 2 - Кадр №\n",
    "### ... - Кадр №\n",
    "### 29 - Кадр №\n",
    "## 1 - видео №\n",
    "## 2 - видео №\n",
    "## ... - - видео №\n",
    "## 29 - видео №\n",
    "# thanks - папка с действие \n",
    "#... - видео №\n",
    "# I love you - папка с действие \n",
    "\n",
    "#Прим. Каждый кадр содержит 468*3+33*4+21*3+21*3 = 1662 landmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb45ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        # try except блок нужен т.к. если у нас уже есть папки с действиями, то вылетет ошибка.\n",
    "        #Поэтому если эти папки уже созданы, то мы просто скипнем действие цикла\n",
    "        #Прим. Папки создаются в папке, где хранится jupiter тетрадка\n",
    "        try: \n",
    "            os.makedirs # создает папку\n",
    "            os.path.join # создает папку DATA_PATH с подпапкой action с подпапкой str(sequence)\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ad5b2",
   "metadata": {},
   "source": [
    "# Сбор данных для обучения и теста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fddf3",
   "metadata": {},
   "source": [
    "Для этого переработаем уже написанный цикл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e59378",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Инициализируем mediapipe модель\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # Новый цикл\n",
    "    #Вместо того, чтобы просто снимать видео с камеры мы создадим цикл, который проходится по каждом кадру в действии\n",
    "    # Для каждого действия...\n",
    "    #Важно: в процессе записи нужно перепробовать различные углы. А также, если жест включает одну руку, то снять половину кадров\n",
    "    #с другой рукой тоже. Также важно держать все участвующие в жесте части тела внутри кадра\n",
    "    #Однако если работаешь с вебкой, то так сильно лучше не эксперементировать. \n",
    "    for action in actions:\n",
    "        # Для каждого видео...\n",
    "        for sequence in range(no_sequences):\n",
    "            # Для каждого кадра в видео...\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Считываем изображение\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Создает детекцию точек\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                #print(results)\n",
    "\n",
    "                # Рисует landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # Добавим немного логики в цикл, чтобы цикл немного ждал перед тем как делать кадры для нового видео \n",
    "                if frame_num == 0: #Если кадр является самым первым, то (мы берем перерыв)\n",
    "                    \n",
    "                    #Далее строчки, которые выводят текст с информацией для нас \n",
    "                    #Первый аргумнет - куда будет поступать текст \n",
    "                    #Второй аргумент - текст \n",
    "                    #Третий аргумнет - позиция (x, y)\n",
    "                    #Четверый арг - шрифт\n",
    "                    #Пятый арг - размер шрифта\n",
    "                    #Шестой арг - цвет текст (B, G, R)\n",
    "                    #Седьмой арг - ширина линии в буквах\n",
    "                    #Восьмой арг - Тип линии\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000) #Длина перерыва, который берем, когда у нас 0-ой кадр\n",
    "                    #Пока на экране STARTING COLLECTION видео не записывается и ты можешь подготовиться к тому, чтобы начать\n",
    "                    #действие. Как только надпись уйдет, то начнется запись для сохранения данных\n",
    "                    \n",
    "                else: #Если кадр не самый первый, то во время видео будет текст, который сообщает о том, для чего \n",
    "                    #собираем данные\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    # Показывает изображение\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # Экспортируем найденные точки в папки\n",
    "                keypoints = extract_keypoints(results) #выгружает точки с кадра в array\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num)) #указывает путь куда экспортировать\n",
    "                np.save(npy_path, keypoints) #сохраняет np.array 1 - куда, 2 - что\n",
    "\n",
    "                # Прекращаем съемку\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab87340",
   "metadata": {},
   "source": [
    "# Предподготовка данных, создание лейблов и признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d881c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем нужные библиотеки\n",
    "from sklearn.model_selection import train_test_split #разделение на обучающую и тестовую выборки\n",
    "from tensorflow.keras.utils import to_categorical #конвертирование one-hot-encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6772452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создадим словарь с лейблами для каждого отдельного действия\n",
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93eb57dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thanks': 1, 'iloveyou': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7915df0",
   "metadata": {},
   "source": [
    "Теперь перейдем к объединению и структуризации данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "224ec09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Мы хотим создать словарь, где для каждого класса есть array 30 видео, который содержит 30 кадров, где в каждом кадре \n",
    "#1662 значения (Landmarks частей тела * len(количество измерений))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "990d149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], [] #два пустых array, который заполним значениями. sequences - признаки(X), labels - таргеты(y)\n",
    "for action in actions: #Для каждого действия в действиях \n",
    "    for sequence in range(no_sequences): #Для каждого видео \n",
    "        window = [] #Для каждого видео здесь будут кадры\n",
    "        for frame_num in range(sequence_length): #Для каждого кадра в видео\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))) #загружаем кадр\n",
    "            window.append(res) #добавляем кадр в window\n",
    "        sequences.append(window) #Для каждого действия добавятся видео с кадрами\n",
    "        labels.append(label_map[action]) #таргеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ea19f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30, 1662)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#получили array в котором 90 видео, в котром 30 кадров и в каждом кадре 1662 значения\n",
    "np.array(sequences).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeec7493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "314fc4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34b34983",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int) #переводим таргеты в one-hot-encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e933322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc411fd",
   "metadata": {},
   "source": [
    "# Создание и обучение LSTM Нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d0c2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a3eeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dee16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(30,1662))) #input = 30 кадров по 1662 точки\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "328691b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01e2facc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3/3 [==============================] - 16s 121ms/step - loss: 1.1152 - categorical_accuracy: 0.3059\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 1.1054 - categorical_accuracy: 0.3412\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.0963 - categorical_accuracy: 0.3529\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.0603 - categorical_accuracy: 0.5294\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.9385 - categorical_accuracy: 0.7059\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.9215 - categorical_accuracy: 0.5294\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.8839 - categorical_accuracy: 0.5176\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.7267 - categorical_accuracy: 0.6353\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6780 - categorical_accuracy: 0.7412\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6207 - categorical_accuracy: 0.7882\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6107 - categorical_accuracy: 0.6353\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6537 - categorical_accuracy: 0.7176\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 1.0738 - categorical_accuracy: 0.4588\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.7787 - categorical_accuracy: 0.5294\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6688 - categorical_accuracy: 0.6588\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6502 - categorical_accuracy: 0.6118\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6105 - categorical_accuracy: 0.6706\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5429 - categorical_accuracy: 0.6706\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5367 - categorical_accuracy: 0.6588\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5363 - categorical_accuracy: 0.6941\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5147 - categorical_accuracy: 0.6824\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5095 - categorical_accuracy: 0.7294\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6035 - categorical_accuracy: 0.6118\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 1.0118 - categorical_accuracy: 0.5412\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.7892 - categorical_accuracy: 0.6118\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.9620 - categorical_accuracy: 0.4353\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.7411 - categorical_accuracy: 0.5647\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5907 - categorical_accuracy: 0.6824\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5661 - categorical_accuracy: 0.6588\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5466 - categorical_accuracy: 0.6353\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5226 - categorical_accuracy: 0.6471\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5039 - categorical_accuracy: 0.7647\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4917 - categorical_accuracy: 0.6941\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4783 - categorical_accuracy: 0.6941\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4712 - categorical_accuracy: 0.6941\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4639 - categorical_accuracy: 0.6941\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4585 - categorical_accuracy: 0.7412\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4516 - categorical_accuracy: 0.9412\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4446 - categorical_accuracy: 0.8000\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4492 - categorical_accuracy: 0.7765\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4672 - categorical_accuracy: 0.7176\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4377 - categorical_accuracy: 0.8353\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4330 - categorical_accuracy: 0.7529\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5106 - categorical_accuracy: 0.9059\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.3952 - categorical_accuracy: 0.8588\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3711 - categorical_accuracy: 0.8353\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3165 - categorical_accuracy: 0.8588\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.2349 - categorical_accuracy: 0.9765\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.2903 - categorical_accuracy: 0.8941\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1900 - categorical_accuracy: 0.9529\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.2021 - categorical_accuracy: 0.9529\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4501 - categorical_accuracy: 0.8824\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2057 - categorical_accuracy: 0.9412\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6420 - categorical_accuracy: 0.7294\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.9068 - categorical_accuracy: 0.6118\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4683 - categorical_accuracy: 0.7647\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.3124 - categorical_accuracy: 0.9176\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3813 - categorical_accuracy: 0.7882\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3030 - categorical_accuracy: 0.8824\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.3058 - categorical_accuracy: 0.8471\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.2088 - categorical_accuracy: 0.9882\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.1711 - categorical_accuracy: 0.9647\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.1405 - categorical_accuracy: 0.9647\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1285 - categorical_accuracy: 0.9765\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3724 - categorical_accuracy: 0.8588\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.7607 - categorical_accuracy: 0.6000\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4798 - categorical_accuracy: 0.7882\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4111 - categorical_accuracy: 0.7765\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4698 - categorical_accuracy: 0.7294\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3440 - categorical_accuracy: 0.8235\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3687 - categorical_accuracy: 0.8000\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3046 - categorical_accuracy: 0.8941\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.3344 - categorical_accuracy: 0.8235\n",
      "Epoch 74/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3191 - categorical_accuracy: 0.8235\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2408 - categorical_accuracy: 0.9176\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.2039 - categorical_accuracy: 0.9412\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1511 - categorical_accuracy: 0.9529\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.1254 - categorical_accuracy: 0.9529\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.1057 - categorical_accuracy: 0.9882\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.0604 - categorical_accuracy: 0.9882\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0435 - categorical_accuracy: 0.9882\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0548 - categorical_accuracy: 0.9765\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0338 - categorical_accuracy: 0.9882\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.0827 - categorical_accuracy: 0.9765\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0725 - categorical_accuracy: 0.9529\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0277 - categorical_accuracy: 0.9882\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0381 - categorical_accuracy: 0.9882\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0582 - categorical_accuracy: 0.9765\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0760 - categorical_accuracy: 0.9765\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0765 - categorical_accuracy: 0.9765\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.0478 - categorical_accuracy: 0.9882\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0136 - categorical_accuracy: 1.0000\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0141 - categorical_accuracy: 1.0000\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0194 - categorical_accuracy: 1.0000\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0204 - categorical_accuracy: 0.9882\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0118 - categorical_accuracy: 1.0000\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0106 - categorical_accuracy: 1.0000\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0066 - categorical_accuracy: 1.0000\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0093 - categorical_accuracy: 1.0000\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0057 - categorical_accuracy: 1.0000\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0081 - categorical_accuracy: 1.0000\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0041 - categorical_accuracy: 1.0000\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0099 - categorical_accuracy: 1.0000\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0080 - categorical_accuracy: 1.0000\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0185 - categorical_accuracy: 0.9882\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0976 - categorical_accuracy: 0.9647\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0829 - categorical_accuracy: 0.9765\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0445 - categorical_accuracy: 0.9882\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0170 - categorical_accuracy: 0.9882\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0096 - categorical_accuracy: 1.0000\n",
      "Epoch 111/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0106 - categorical_accuracy: 1.0000\n",
      "Epoch 112/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0106 - categorical_accuracy: 1.0000\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0120 - categorical_accuracy: 1.0000\n",
      "Epoch 114/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0164 - categorical_accuracy: 0.9882\n",
      "Epoch 115/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0089 - categorical_accuracy: 1.0000\n",
      "Epoch 116/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0158 - categorical_accuracy: 0.9882\n",
      "Epoch 117/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0076 - categorical_accuracy: 1.0000\n",
      "Epoch 118/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0135 - categorical_accuracy: 1.0000\n",
      "Epoch 119/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0350 - categorical_accuracy: 0.9882\n",
      "Epoch 120/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0033 - categorical_accuracy: 1.0000\n",
      "Epoch 121/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0058 - categorical_accuracy: 1.0000\n",
      "Epoch 122/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0031 - categorical_accuracy: 1.0000\n",
      "Epoch 123/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0021 - categorical_accuracy: 1.0000\n",
      "Epoch 124/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0025 - categorical_accuracy: 1.0000\n",
      "Epoch 125/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0025 - categorical_accuracy: 1.0000\n",
      "Epoch 126/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0021 - categorical_accuracy: 1.0000\n",
      "Epoch 127/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0019 - categorical_accuracy: 1.0000\n",
      "Epoch 128/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0017 - categorical_accuracy: 1.0000\n",
      "Epoch 129/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0017 - categorical_accuracy: 1.0000\n",
      "Epoch 130/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0016 - categorical_accuracy: 1.0000\n",
      "Epoch 131/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0016 - categorical_accuracy: 1.0000\n",
      "Epoch 132/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0015 - categorical_accuracy: 1.0000\n",
      "Epoch 133/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.0014 - categorical_accuracy: 1.0000\n",
      "Epoch 134/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0013 - categorical_accuracy: 1.0000\n",
      "Epoch 135/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0013 - categorical_accuracy: 1.0000\n",
      "Epoch 136/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0012 - categorical_accuracy: 1.0000\n",
      "Epoch 137/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0012 - categorical_accuracy: 1.0000\n",
      "Epoch 138/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0012 - categorical_accuracy: 1.0000\n",
      "Epoch 139/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0011 - categorical_accuracy: 1.0000\n",
      "Epoch 140/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0011 - categorical_accuracy: 1.0000\n",
      "Epoch 141/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0011 - categorical_accuracy: 1.0000\n",
      "Epoch 142/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0010 - categorical_accuracy: 1.0000\n",
      "Epoch 143/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0010 - categorical_accuracy: 1.0000\n",
      "Epoch 144/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 9.8393e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 145/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 9.6089e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 146/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 9.3928e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 29ms/step - loss: 9.1956e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 148/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 8.9958e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 149/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 8.7950e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 150/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 8.6120e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 151/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 8.4519e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 152/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 8.2800e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 153/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 8.1172e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 154/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 7.9731e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 155/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 7.8212e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 156/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 7.6652e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 157/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 7.5403e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 158/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 7.4004e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 159/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 7.2710e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 160/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 7.1453e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 161/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 7.0215e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 162/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 6.9053e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 163/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 6.7840e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 164/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 6.6744e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 165/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 6.5636e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 166/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 6.4610e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 167/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 6.3588e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 168/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 6.2507e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 169/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 6.1536e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 170/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 6.0486e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 171/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 5.9527e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 172/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 5.8528e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 173/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 5.7467e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 174/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 5.6429e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 175/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 5.5261e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 176/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 5.4090e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 177/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 5.3040e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 178/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 5.1946e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 179/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 5.0848e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 180/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 4.9885e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 181/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 4.8859e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 182/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 4.7949e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 183/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.7030e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 184/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 4.6163e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 185/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.5291e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 186/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.4523e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 187/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 4.3708e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 188/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 4.2953e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 189/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 4.2230e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 190/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 4.1525e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 191/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 4.0846e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 192/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 4.0176e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 193/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.9527e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 194/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.8946e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 195/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 3.8332e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 196/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 3.7743e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 197/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.7162e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 198/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 3.6607e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 199/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.6103e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 200/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 3.5548e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 201/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 3.5064e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 202/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 3.4576e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 203/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 3.4084e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 204/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.3614e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 205/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 3.3150e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 206/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.2703e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 207/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 3.2261e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 208/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.1874e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 209/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.1442e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 210/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 3.1034e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 211/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 3.0646e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 212/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.0256e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 213/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.9874e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 214/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.9508e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 215/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.9140e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 216/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.8783e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 217/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 24ms/step - loss: 2.8455e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 218/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.8101e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 219/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.7768e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 220/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.7448e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 221/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.7129e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 222/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.6807e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 223/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.6507e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 224/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.6198e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 225/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.5905e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 226/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.5620e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 227/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.5327e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 228/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.5039e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 229/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.4778e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 230/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.4506e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 231/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.4244e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 232/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.3985e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 233/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.3729e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 234/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.3476e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 235/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.3233e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 236/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.2996e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 237/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.2759e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 238/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.2525e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 239/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.2290e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 240/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.2060e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 241/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.1838e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 242/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.1626e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 243/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.1407e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 244/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.1199e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 245/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.0979e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 246/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.0786e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 247/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.0581e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 248/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 2.0375e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 249/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 2.0181e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 250/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.9994e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 251/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.9798e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 252/300\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 1.9611e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 253/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 1.9432e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 254/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.9251e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 255/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.9068e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 256/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.8892e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 257/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 1.8720e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 258/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 1.8553e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 259/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.8385e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 260/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.8218e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 261/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 1.8053e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 262/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.7899e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 263/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 1.7737e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 264/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.7587e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 265/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 1.7429e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 266/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.7284e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 267/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.7129e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 268/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.6993e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 269/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.6838e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 270/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 1.6699e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 271/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 1.6559e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 272/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.6417e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 273/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.6281e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 274/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.6143e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 275/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.6009e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 276/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 1.5876e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 277/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.5747e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 278/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.5621e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 279/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.5492e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 280/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.5366e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 281/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.5240e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 282/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.5117e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 283/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.4996e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 284/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.4874e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 285/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 1.4754e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 286/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 1.4628e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 287/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 28ms/step - loss: 1.4517e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 288/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.4404e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 289/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 1.4298e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 290/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.4180e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 291/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 1.4072e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 292/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.3966e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 293/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.3858e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 294/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.3748e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 295/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.3644e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 296/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.3539e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 297/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.3439e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 298/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.3334e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 299/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.3237e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 300/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 1.3137e-04 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16d4b932e80>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=300, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "668da6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,675\n",
      "Trainable params: 596,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96e45a",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e9ed200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'iloveyou'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(model.predict(X_test)[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca25dd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iloveyou'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f5ecae",
   "metadata": {},
   "source": [
    "# Сохранение модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceaa12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "034512a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5') #Таким образом можно загрузить веса, которые выработала модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2568e",
   "metadata": {},
   "source": [
    "# Оценка модели с помощью Confusion Matrix (Матрица ошибок) и Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9ad46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d4fe9",
   "metadata": {},
   "source": [
    "Confusion Matrix выдает для каждого таргета значения, которые оказались TP, TN, FP, FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95a21d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0043a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist() #настоящие таргеты \n",
    "yhat = np.argmax(yhat, axis=1).tolist() #предсказанные таргеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b07513b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[1, 0],\n",
       "        [0, 4]]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)\n",
    "#Если нужно больше узнать об conf_matrxi, то можно вбить ячейке multilabel_confusion_matrix??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e51b7e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efbfcdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e07ec",
   "metadata": {},
   "source": [
    "# Предсказание в реальном времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8df6ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создадим функцию для дополнительной визуализации\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)] #цвета для каждого действия \n",
    "def prob_viz(res, actions, input_frame, colors): #знакомые аргументы на входе\n",
    "    #input_frame - получает изображение\n",
    "    output_frame = input_frame.copy() #чтобы не менять исходник берем его копию\n",
    "    for num, prob in enumerate(res): #для каждого действия подсвечивается вероятность того, что было показано именно оно\n",
    "        #Первый арг - изображение на вход, куда помещается rectangle\n",
    "        #Второй арг - позиция прямоугольника меняется от дейстия\n",
    "        #Третий арг - меняет длину подсветки в слово в зависимости от уверенности модели в данном слове\n",
    "        #Четверый арг - \n",
    "        #Пятый арг - Цвет\n",
    "        #Шестой арг - Помещает это в \"коробку\"\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98eef9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Новые данные для детекции\n",
    "#Чтобы понять логику sentece нужно уточнить, что мы занимаемся расшифровкой не для отдельных значений, а для языка жестов.\n",
    "#мы хотим чтобы модель считывала наши жесты, показывала какой это жест, мы его конвертируем в слово, а из слов состовляем\n",
    "#предложение.\n",
    "sequence = [] #собирает 30 кадров. Пока открыта Open CV, сюда мы будем собирать 30 кадров, которые пойдут на предсказание \n",
    "sentence = [] #Разберем далее (Пер. с англ \"предложение\")\n",
    "predictions = [] #сюда добавляем предсказания \n",
    "threshold = 0.9 # Метрика уверенности. Мы будем рендерить результаты только, если модель уверена в своем решении более, чем\n",
    "#на threshold \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # Логика для предсказания\n",
    "        #Важно помнить, что предсказание делается на основе 30 кадров. Сначала мы объединяем кадры в листе sequence.\n",
    "        #Как только у нас будет 30 кадров, то мы сможем сделать предсказание \n",
    "        keypoints = extract_keypoints(results) #считывает координаты точек\n",
    "        \n",
    "        #то же самое, что и снизу, но \"другими словами\"\n",
    "        #sequence.insert(0,keypoints)\n",
    "        #sequence = sequence[:30]\n",
    "        \n",
    "        #это используем\n",
    "        sequence.append(keypoints) #добавляем координаты точек в sequence\n",
    "        sequence = sequence[-30:] #Теперь берем только последние 30 кадров. На основе последних 30 кадров модель будет делать\n",
    "        #предсказание, т.к. в начальных кадрах мы можем делать другие вещи.\n",
    "        \n",
    "        if len(sequence) == 30: #если кадров == 30\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0] #делаем предсказание\n",
    "            #используем np.expand_dims, т.к. у X_train форам (30, 1662), а модель ожидает на вход (1, 30, 1662). Таким образом\n",
    "            #мы подгоняем array под нужную форму \n",
    "            print(actions[np.argmax(res)]) #показывает что это было за значение\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Логика для визуализации\n",
    "        #предсказания появляются мгновенно и иногда, когда мы начинаем показывать hello, то сначала предсказывается iloveyou\n",
    "        #поэтому мы берем только последние 10 предсказаний, берем уникальное значение из них, которое равно\n",
    "        #будет одному предсказанию (если не будет равно одному предсказанию, то тогда не сработает визуализация) и сравниваем\n",
    "        #с предсказанием.\n",
    "        #Получается, что мы чекаем, что последние 10 кадров дают тоже самое предсказание, что и последнее предсказание.\n",
    "        #Мы вязли последние 10 кадров, т.к. в последние 10 кадров жест останавливает движение и приобретает свою законченную\n",
    "        #форму. \n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res):\n",
    "                if res[np.argmax(res)] > threshold: #Если уверенность модель в предсказании больше threshold\n",
    "                    #Наши предсказания в конечном итоге преобретают форму слов (пр. Hello)\n",
    "                    #В sentence мы добавляем предсказания в форме слов\n",
    "                    if len(sentence) > 0: #Если в предсказаниях уже что-то есть, то...\n",
    "                        if actions[np.argmax(res)] != sentence[-1]: #Если новое предсказание не равно старому (последнему) предсказанию\n",
    "                            #Мы делаем это для того, чтобы модель постоянно не показывала нам предсказание, когда мы используем \n",
    "                            #один и тот же жест\n",
    "                            sentence.append(actions[np.argmax(res)]) #добавляем предсказание в лист\n",
    "\n",
    "                    #если в предсказаниях ничего нет, то добавляем любое первое первое предсказание каким бы оно ни было\n",
    "                    #(речь не про уверенность)\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)]) \n",
    "                    \n",
    "            #Модель постоянно предсказывает наши жесты. Чтобы не выгружать огромный array, то мы возьмем только последние\n",
    "            #5 предсказаний\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    " \n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "        \n",
    "        #перейдем к рендеру. Мы будем визуализировать слова, которые мы показали жестами\n",
    "        #формируем место для текста\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)  \n",
    "        #вставляем текст\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.4",
   "language": "python",
   "name": "tf2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
