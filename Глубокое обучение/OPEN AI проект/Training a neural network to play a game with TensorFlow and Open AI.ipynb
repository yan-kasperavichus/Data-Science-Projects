{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a8d773",
   "metadata": {},
   "source": [
    "# Обучение нейронности сети игре с помощью TensorFlow и OPEN AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469c3a8",
   "metadata": {},
   "source": [
    "Целью проекта является демонстрация навыков в обучении с подкреплением. Целью не является создание модели высокого уровня, хотя при должном желании, проблема могла бы быть решена. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1fa41",
   "metadata": {},
   "source": [
    "Одна из больших проблем в обучении НС - это недостаток данных. В случае же с физикой мы можем создать уникальные данные, которые будут основаны на статистике и знании о явлении, поэтому мы можем искусственно сгенерировать всевозможный датасет для обучения НС. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153704a",
   "metadata": {},
   "source": [
    "Еще один пример формирования сценария данных - это использование Open AI (Gym). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f794bea",
   "metadata": {},
   "source": [
    "gym.openai.com - сайт, которые включает в себя множество возможных сред для тренировки ИИ. Мы воспользуемся средой CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd766e72",
   "metadata": {},
   "source": [
    "НС будет создавать сигналы, которые будут подаваться на игровую модель. В данном случае будет использоваться метод амсаблирования, когда несколько слабых моделей в совокупности будут давать более правильный (сильный) сигнал. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddad6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym #формирует среду для тренировки модели \n",
    "import random #в игре вертикальная плашка движется рандомно. Использовав random мы сможем сгенерировать данные, похожие\n",
    "#на движение плашки\n",
    "import numpy as np\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected \n",
    "from tflearn.layers.estimator import regression \n",
    "from statistics import mean, median \n",
    "from collections import Counter \n",
    "import tflearn\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dc9093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da7f1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#не дает модели использовать более 0.333 оперативки, помогает если надо обучать несколько моделей сразу (к примеру, когда\n",
    "#мы учили несколько моделей сразу играть в игру)\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.99)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "102a5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3 #learning_rate\n",
    "env = gym.make('CartPole-v1') #окружение - игра, которой обучаем модель\n",
    "env.reset()\n",
    "goal_steps = 500 #Движение в игре происходит в кадрах. За каждый кадр, когда модель\n",
    "#удерживает вертикальную линию мы получаем очки. goal_steps - количество кадров (перев. шагов до достижения цели). \n",
    "#сколько шагов мы даем модели для достижения цели\n",
    "score_requirement = 50 #минимальное количество очков, которое должна набрать модель\n",
    "initial_games = 10000 #количество игр, которые сыграет модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50f14ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#для начала будем создавать игры с рандомными данными:\n",
    "def some_random_games_first():\n",
    "    for episode in range(100): #episode - эпизод игры\n",
    "        env.reset() #обновляем среду\n",
    "        for t in range(goal_steps): # для каждого кадра из диапазона...\n",
    "            env.render() #если хочешь видеть, что происходит в игре. Естественно, это замедлит обучение. Если не хочешь\n",
    "            #видеть, то просто закомментируй\n",
    "            action = env.action_space.sample() #действия, которые может совершать модель\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            #observation - array из данных об игре. В данном случае просто позиция модели в каждом кадре.\n",
    "            #reward - 1 или 0. Балансировала ли модель. Если да, то 1.\n",
    "            #done - конец игры\n",
    "            #info - другая информация\n",
    "            if done:\n",
    "                break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ed9e4",
   "metadata": {},
   "source": [
    "Когда у нас есть окружение, которое мы можем генерировать таким путем, мы можем генерировать сэмплы для тренировки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a01aef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_population(): #функция для создания данных\n",
    "    training_data = [] #здесь будет observation и движения, которые были сделаны\n",
    "    #все данные будут рандомные, но мы будем добавлять их в training_data только если очки за игру будут выше score_requirement\n",
    "    scores = [] \n",
    "    accepted_scores = []\n",
    "    \n",
    "    for _ in range(initial_games): #для каждой игры, которую сыграет модель\n",
    "        score = 0 #начальное количество очков\n",
    "        game_memory = [] #до конца самой игры мы не будем знать свое количество очков, поэтому нам придется сохраняться\n",
    "        #движения каждой модели \n",
    "        prev_observation = [] #лист с данными о прошлых играх \n",
    "        \n",
    "        for _ in range(goal_steps): #для каждого кадра в игре. В игре всего будет goal_steps кадров \n",
    "            action = random.randrange(0,2) #генерирует только 0 и 1. Действия, которые совершает модель. Т.к. у модели только\n",
    "            #два пути: двигаться влево или вправо. \n",
    "            observation, reward, done, info = env.step(action) #генерируем данные об шаге для каждой игры\n",
    "            \n",
    "            if len(prev_observation) > 0: #если в листе prev_observation уже есть наблюдения, то\n",
    "                game_memory.append([prev_observation, action]) #позция модели в прошлом кадре и её действие\n",
    "                #позиция в прошлом кадре, т.к. сначала obeservation зависит от env.step.action и сначала генерируется\n",
    "                #действие потом, потом позиция модели в кадре\n",
    "            \n",
    "            #иначе, если в листе prev_observation нет наблюдений, туда попадает наблюдение, созданное первоначально\n",
    "            prev_observation = observation\n",
    "            \n",
    "            score += reward #если модель сбалансировала в данном кадре, то прибавляется очко (которых нам надо \n",
    "            #набрать score_requirement)\n",
    "            \n",
    "            if done: #если все кадры (goal_steps) сыграны, то закончить\n",
    "                break #заканчиваем цикл\n",
    "                \n",
    "        #теперь анализируем проведенную игру. Набрала ли модель нужно кол-во очков?\n",
    "        #Для каждой сыгранной игры:\n",
    "        if score >= score_requirement: #если набранное количество очков за игру больше требуемого\n",
    "            accepted_scores.append(score) #мы принимаем очки и добавляем их в accepted_scores\n",
    "            #если мы модель набрала больше, чем score_requirement очков, то записываем её действия\n",
    "            for data in game_memory: #для данных об каждой игре\n",
    "                #здесь создаются вектора движения. Т.к. у нас 2 действия, то конвертируем их в one-hot-enc\n",
    "                #если модель action=1, то вектор [0, 1] и т.д. В общем это просто векторезированное действие.\n",
    "                if data[1] == 1: #data[1] = action. Если action (котрое делали рандомно) = 1, то \n",
    "                    output = [0, 1] \n",
    "                elif data[1] == 0:\n",
    "                    output = [1, 0]\n",
    "                    \n",
    "                training_data.append([data[0], output]) #добавляем в даные об каждой игре \n",
    "                #data[0] = prev_observation - данные о движение модели\n",
    "                \n",
    "        env.reset() #когда игра закончена обновляем окружение\n",
    "        scores.append(score) #просто чтобы видеть все очки, которых модель достигла\n",
    "        \n",
    "    #игры закончились.\n",
    "    training_data_save = np.array(training_data) #сохраняем данные и конвертируем их в array\n",
    "    np.save('saved.np', training_data_save) \n",
    "    \n",
    "\n",
    "\n",
    "    #отдел аналитик\n",
    "    print('Average accepted score:', mean(accepted_scores))\n",
    "    print('Median accepted score', median(accepted_scores))\n",
    "    print(Counter(accepted_scores)) #показывает сколько раз модель достигла определенного очка\n",
    "    #пр. 65.0 : 2, значит модель два раза достигла 65 очков\n",
    "    \n",
    "    return training_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f519a22",
   "metadata": {},
   "source": [
    "С помощью tf можно сохранять модель, но чтобы загрузить эту сохраненную модель, нам нужно иметь модель, которая уже была инициализированна и на инпуты в нее должны заходить идентичные формы данных. Так что лучше разделять саму модель, её тренировку и её использование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b828144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализируем модель:\n",
    "def neural_network_model(input_size):\n",
    "    network = input_data(shape = [None, input_size, 1], name = 'input') #input_size в нашем случае 0, т.к. это observation.\n",
    "    \n",
    "    #создадим полносвязный лэйер\n",
    "    network = fully_connected(network, 128, activation='relu')\n",
    "    network = dropout(network, 0.8) #0.8 - это keep_rate, а не drop_rate. Drop_rate по дефолту также остается 0.2\n",
    "    \n",
    "    network = fully_connected(network, 256, activation='relu')\n",
    "    network = dropout(network, 0.8) #0.8 - это keep_rate, а не drop_rate. Drop_rate по дефолту также остается 0.2\n",
    "    \n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.8) #0.8 - это keep_rate, а не drop_rate. Drop_rate по дефолту также остается 0.2\n",
    "    \n",
    "    network = fully_connected(network, 256, activation='relu')\n",
    "    network = dropout(network, 0.8) #0.8 - это keep_rate, а не drop_rate. Drop_rate по дефолту также остается 0.2\n",
    "    \n",
    "    network = fully_connected(network, 128, activation='relu')\n",
    "    network = dropout(network, 0.8) #0.8 - это keep_rate, а не drop_rate. Drop_rate по дефолту также остается 0.2\n",
    "    \n",
    "    network = fully_connected(network, 2, activation='softmax') #2 - это количество опций модели куда двигаться\n",
    "    #А что если в игре больше двух действий? Поговорим об этом ниже в markdown ячейке\n",
    "    \n",
    "    network = regression(network, optimizer = 'adam',\n",
    "                         learning_rate=1e-3,\n",
    "                         loss='categorical_crossentropy',\n",
    "                         name = 'targets') \n",
    "    #задача регресси т.к. мы предсказываем на сколько \n",
    "    #должна двинуться модель, а не просто куда\n",
    "    \n",
    "    model = tflearn.DNN(network, tensorboard_dir='logs') \n",
    "    #можем сюда добавить подключение к tensorboard через укаазание tensorboard_dir = 'logs '\n",
    "    #tensorboard создаст новую папку logs в месте, где хранится тетредь.\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71fa96",
   "metadata": {},
   "source": [
    "Мы делаем код максимально динамичным, чтобы он мог подстроиться к любой игре на gym. Допустим в игре 4 действия, тогда:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d6e35",
   "metadata": {},
   "source": [
    "1) Изменяем саму игру env = gym.make('игра_name')\n",
    "\n",
    "\n",
    "2) Изменяем score_requirement = ...\n",
    "\n",
    "\n",
    "3) Смотрим сколько действия теперь есть для модели в action = env.action_space.sample()\n",
    "\n",
    "\n",
    "4) Изменяем action = random.randrange(..., ...) на кол-во новых возможных действий \n",
    "\n",
    "\n",
    "5) network = fully_connected(network, ..., ) меняем кол-во аутпутов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4487801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для тренировки модели\n",
    "def train_model(training_data, model=False):\n",
    "    #если модель уже создана, то мы просто передадим её в функцию. Если модели еще нет, то фукнция создаст модель\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]), 1)\n",
    "    #-1 - мы \"говорим\" numpy о том, что хотим изменить форму array таким образом, чтобы он подходил под критерий \n",
    "    #len(training_data[0[0]])\n",
    "    #Таким образом получаем np.array c формой 4 векторов, где есть 4 строки, 1 колонка, а векторы обернуты в 2 квадратных скоб.\n",
    "    #training_data[0] - observation (движения модели в кадре)\n",
    "    y = [i[1] for i in training_data]\n",
    "    \n",
    "    #если у нас еще нет модели, то\n",
    "    if not model:\n",
    "        model = neural_network_model(input_size=len(X[0]))\n",
    "        \n",
    "    #не стоит ставить много эпох, т.к. модель может переучиться. Если у нас на этапе обучения будет 95% accuracy, то скорее всего\n",
    "    #у нас будет переобучение\n",
    "    model.fit({'input':X}, {'targets':y}, n_epoch=4, snapshot_step=500, show_metric=True, run_id='openaistaff')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77307c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 62.569060773480665\n",
      "Median accepted score 58.0\n",
      "Counter({52.0: 30, 50.0: 28, 51.0: 26, 56.0: 24, 53.0: 21, 55.0: 21, 60.0: 15, 57.0: 15, 61.0: 13, 54.0: 13, 62.0: 12, 64.0: 12, 58.0: 10, 68.0: 9, 65.0: 9, 59.0: 8, 73.0: 7, 66.0: 7, 63.0: 6, 69.0: 5, 84.0: 4, 75.0: 4, 74.0: 4, 78.0: 4, 90.0: 4, 71.0: 4, 80.0: 3, 92.0: 3, 67.0: 3, 86.0: 3, 87.0: 3, 77.0: 3, 72.0: 3, 82.0: 2, 85.0: 2, 79.0: 2, 120.0: 2, 70.0: 2, 103.0: 2, 88.0: 1, 89.0: 1, 117.0: 1, 109.0: 1, 128.0: 1, 115.0: 1, 100.0: 1, 81.0: 1, 101.0: 1, 95.0: 1, 76.0: 1, 110.0: 1, 104.0: 1, 83.0: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanka\\AppData\\Local\\Temp\\ipykernel_17752\\3666262703.py:52: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training_data_save = np.array(training_data) #сохраняем данные и конвертируем их в array\n"
     ]
    }
   ],
   "source": [
    "training_data = initial_population()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aa7c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.65966\u001b[0m\u001b[0m | time: 4.344s\n",
      "| Adam | epoch: 004 | loss: 0.65966 - acc: 0.6101 -- iter: 22272/22288\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.66332\u001b[0m\u001b[0m | time: 4.356s\n",
      "| Adam | epoch: 004 | loss: 0.66332 - acc: 0.6100 -- iter: 22288/22288\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model = train_model(training_data) #т.к. у нас еще нет модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f25ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#можем сохранить модель\n",
    "#model.save('game_model.model')\n",
    "\n",
    "#если захотим её загрузить позже, то можем просто сделать\n",
    "#model.load(game_model.model)\n",
    "#Таким образом нам не надо переучивать заново модель, запуская код. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd59a5d",
   "metadata": {},
   "source": [
    "Accuracy в 61 процент тоже неплохо, но могло бы быть и лучше. Поработаем над этим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21fc7dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 125.3\n",
      "Choice 1: 0.5107741420590582, Choice 0: 0.48922585794094176\n"
     ]
    }
   ],
   "source": [
    "scores = [] #очки за каждую игру\n",
    "choices = [] #выбор модели куда двигаться\n",
    "\n",
    "for each_game in range(10): #для каждой игры, которые будет range(N)\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_obs = []\n",
    "    env.reset() #ресетим окружение перед началом каждой новой игры\n",
    "    #для каждой игры, которая будет длиться goal_steps кадров\n",
    "    for _ in range(goal_steps):\n",
    "        env.render() #чтобы мы могли видеть игру\n",
    "        \n",
    "        #если нет никаких данных об среде в prev_obs, то создаем действие\n",
    "        if len(prev_obs) == 0:\n",
    "            action = random.randrange(0, 2)\n",
    "        #но когда модель увидит первый кадр (куда движется палка)\n",
    "        else:\n",
    "            #action - должно быть 0 или 1. Аутпут из модели one-hot ([0, 1] или [1, 0])\n",
    "            #через argmax берем индекс самого большого значения от предсказанного действия model.predict(prev_obs.)\n",
    "            #модель видит первый кадр, и показывает действие, куда ей лучше двигаться. \n",
    "            #Х должен поступать на вход в определенной форме. Мы уже делали reshape для трейн данных и нужно сделать\n",
    "            #такой же решейп для новых данных\n",
    "            #берем 0 элемент из предикта модели, т.к. нам нужен ответ только на первый кадр\n",
    "            #model.predict(prev_obs.reshape(-1, len(prev_obs), 1))[0] = array([0.64129007, 0.35870993], dtype=float32)\n",
    "            #в предикте, т.к. мы используем loss=categorical_crossentropy, у нас хранится вероятность того, какой класс\n",
    "            #больше подходит под инпут. 0.64129007 - вероятность принадлежать первом классу, а 0.35870993 - второму\n",
    "            #таким образом мы с помощью argmax берем индекс наибольшей вероятности. \n",
    "            #[0] индекс в конце нужен, т.к. model.predict(prev_obs.reshape(-1, len(prev_obs), 1)) дает \n",
    "            # array([[0.64129007, 0.35870993]], dtype=float32). Проще говоря, мы просто избалвяемся от скобок [].\n",
    "            #Можешь чекнкуть еще раз с помощью \n",
    "            #model.predict(np.array([training_data][0][0][0]).reshape(-1, len(training_data[0][0]), 1))[0]\n",
    "            action = np.argmax(model.predict(prev_obs.reshape(-1, len(prev_obs), 1))[0])\n",
    "            \n",
    "        choices.append(action)\n",
    "        \n",
    "        new_observation, reward, done, info = env.step(action) #здесь данные о каждом шаге\n",
    "        prev_obs = new_observation \n",
    "        #вообще нам не нужна линия ниже, если мы не собираемся доучивать модель.\n",
    "        game_memory.append([new_observation, action]) #записываются данные о текущей игре. Состояние среды и действие модели\n",
    "        #в ответ на среду\n",
    "        score += reward #добавляем очки, которые модель получает за каждый кадр в игре\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    #собираем очки за каждую игру\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Average score:', sum(scores)/len(scores))\n",
    "#интересно посмотреть на сколько раз чаще модель отдает предпочтение первому действию перед вторым. Не происходит ли никакой\n",
    "#ошибки?\n",
    "print('Choice 1: {}, Choice 0: {}'.format(choices.count(1)/len(choices), choices.count(0)/len(choices)))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1b898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.4",
   "language": "python",
   "name": "tf2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
